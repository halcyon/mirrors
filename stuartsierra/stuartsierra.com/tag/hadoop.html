<!DOCTYPE html>
<html lang="en-US" class="no-js">

<!-- Mirrored from stuartsierra.com/tag/hadoop by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 02 Sep 2016 16:56:02 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="profile" href="http://gmpg.org/xfn/11">
		<script>(function(html){html.className = html.className.replace(/\bno-js\b/,'js')})(document.documentElement);</script>
<title>Hadoop &#8211; Digital Digressions by Stuart Sierra</title>
<link rel='dns-prefetch' href='http://s0.wp.com/'>
<link rel='dns-prefetch' href='http://secure.gravatar.com/'>
<link rel='dns-prefetch' href='http://fonts.googleapis.com/'>
<link rel='dns-prefetch' href='http://s.w.org/'>
<link rel="alternate" type="application/rss+xml" title="Digital Digressions by Stuart Sierra &raquo; Feed" href="http://feeds2.feedburner.com/StuartSierra" />
<link rel="alternate" type="application/rss+xml" title="Digital Digressions by Stuart Sierra &raquo; Comments Feed" href="../comments/feed" />
<link rel="alternate" type="application/rss+xml" title="Digital Digressions by Stuart Sierra &raquo; Hadoop Tag Feed" href="http://feeds2.feedburner.com/StuartSierra" />
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/stuartsierra.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.6"}};
			!function(a,b,c){function d(a){var c,d,e,f,g,h=b.createElement("canvas"),i=h.getContext&&h.getContext("2d"),j=String.fromCharCode;if(!i||!i.fillText)return!1;switch(i.textBaseline="top",i.font="600 32px Arial",a){case"flag":return i.fillText(j(55356,56806,55356,56826),0,0),!(h.toDataURL().length<3e3)&&(i.clearRect(0,0,h.width,h.height),i.fillText(j(55356,57331,65039,8205,55356,57096),0,0),c=h.toDataURL(),i.clearRect(0,0,h.width,h.height),i.fillText(j(55356,57331,55356,57096),0,0),d=h.toDataURL(),c!==d);case"diversity":return i.fillText(j(55356,57221),0,0),e=i.getImageData(16,16,1,1).data,f=e[0]+","+e[1]+","+e[2]+","+e[3],i.fillText(j(55356,57221,55356,57343),0,0),e=i.getImageData(16,16,1,1).data,g=e[0]+","+e[1]+","+e[2]+","+e[3],f!==g;case"simple":return i.fillText(j(55357,56835),0,0),0!==i.getImageData(16,16,1,1).data[0];case"unicode8":return i.fillText(j(55356,57135),0,0),0!==i.getImageData(16,16,1,1).data[0];case"unicode9":return i.fillText(j(55358,56631),0,0),0!==i.getImageData(16,16,1,1).data[0]}return!1}function e(a){var c=b.createElement("script");c.src=a,c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i;for(i=Array("simple","flag","unicode8","diversity","unicode9"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='twentysixteen-jetpack-css'  href='../wp-content/plugins/jetpack/modules/theme-tools/compat/twentysixteen3a05.css?ver=4.2.2' type='text/css' media='all' />
<link rel='stylesheet' id='twentysixteen-fonts-css'  href='https://fonts.googleapis.com/css?family=Merriweather%3A400%2C700%2C900%2C400italic%2C700italic%2C900italic%7CMontserrat%3A400%2C700%7CInconsolata%3A400&amp;subset=latin%2Clatin-ext' type='text/css' media='all' />
<link rel='stylesheet' id='genericons-css'  href='../wp-content/plugins/jetpack/_inc/genericons/genericons/genericons128b.css?ver=3.1' type='text/css' media='all' />
<link rel='stylesheet' id='twentysixteen-style-css'  href='../wp-content/themes/twentysixteen/style167b.css?ver=4.6' type='text/css' media='all' />
<!--[if lt IE 10]>
<link rel='stylesheet' id='twentysixteen-ie-css'  href='https://stuartsierra.com/wp-content/themes/twentysixteen/css/ie.css?ver=20160816' type='text/css' media='all' />
<![endif]-->
<!--[if lt IE 9]>
<link rel='stylesheet' id='twentysixteen-ie8-css'  href='https://stuartsierra.com/wp-content/themes/twentysixteen/css/ie8.css?ver=20160816' type='text/css' media='all' />
<![endif]-->
<!--[if lt IE 8]>
<link rel='stylesheet' id='twentysixteen-ie7-css'  href='https://stuartsierra.com/wp-content/themes/twentysixteen/css/ie7.css?ver=20160816' type='text/css' media='all' />
<![endif]-->
<link rel='stylesheet' id='jetpack_css-css'  href='../wp-content/plugins/jetpack/css/jetpack3a05.css?ver=4.2.2' type='text/css' media='all' />
<script type='text/javascript' src='../wp-includes/js/jquery/jqueryb8ff.js?ver=1.12.4'></script>
<script type='text/javascript' src='../wp-includes/js/jquery/jquery-migrate.min330a.js?ver=1.4.1'></script>
<!--[if lt IE 9]>
<script type='text/javascript' src='https://stuartsierra.com/wp-content/themes/twentysixteen/js/html5.js?ver=3.7.3'></script>
<![endif]-->
<link rel='https://api.w.org/' href='../wp-json/index.html' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../xmlrpc0db0.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 4.6" />

<link rel='dns-prefetch' href='http://v0.wordpress.com/'>
</head>

<body class="archive tag tag-hadoop tag-76 hfeed">
<div id="page" class="site">
	<div class="site-inner">
		<a class="skip-link screen-reader-text" href="#content">Skip to content</a>

		<header id="masthead" class="site-header" role="banner">
			<div class="site-header-main">
				<div class="site-branding">
					
											<p class="site-title"><a href="../index.html" rel="home">Digital Digressions by Stuart Sierra</a></p>
											<p class="site-description">From programming to everything else</p>
									</div><!-- .site-branding -->

									<button id="menu-toggle" class="menu-toggle">Menu</button>

					<div id="site-header-menu" class="site-header-menu">
													<nav id="site-navigation" class="main-navigation" role="navigation" aria-label="Primary Menu">
								<div class="menu-main-menu-container"><ul id="menu-main-menu" class="primary-menu"><li id="menu-item-635" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-635"><a href="../about-me.html">About Me</a></li>
<li id="menu-item-633" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-633"><a href="../writing.html">Writing</a></li>
<li id="menu-item-634" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-634"><a href="../presentations.html">Presentations</a></li>
<li id="menu-item-636" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-636"><a href="../software.html">Software</a></li>
<li id="menu-item-637" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-637"><a href="../contact.html">Contact</a></li>
</ul></div>							</nav><!-- .main-navigation -->
						
											</div><!-- .site-header-menu -->
							</div><!-- .site-header-main -->

											<div class="header-image">
					<a href="../index.html" rel="home">
						<img src="../wp-content/uploads/2015/12/header.jpg" srcset="https://stuartsierra.com/wp-content/uploads/2015/12/header-300x70.jpg 300w, https://stuartsierra.com/wp-content/uploads/2015/12/header-768x179.jpg 768w, https://stuartsierra.com/wp-content/uploads/2015/12/header-1024x239.jpg 1024w, https://stuartsierra.com/wp-content/uploads/2015/12/header.jpg 1200w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 81vw, (max-width: 1362px) 88vw, 1200px" width="1200" height="280" alt="Digital Digressions by Stuart Sierra">
					</a>
				</div><!-- .header-image -->
					</header><!-- .site-header -->

		<div id="content" class="site-content">

	<div id="primary" class="content-area">
		<main id="main" class="site-main" role="main">

		
			<header class="page-header">
				<h1 class="page-title">Tag: Hadoop</h1>			</header><!-- .page-header -->

			
<article id="post-953" class="post-953 post type-post status-publish format-standard hentry category-programming tag-hadoop tag-open-source">
	<header class="entry-header">
		
		<h2 class="entry-title"><a href="../2016/07/18/apathy-of-the-commons.html" rel="bookmark">Apathy of the Commons</a></h2>	</header><!-- .entry-header -->

	
	
	<div class="entry-content">
		<p>Eight years ago, I filed a bug on an open-source project.</p>
<p><a href="https://issues.apache.org/jira/browse/HADOOP-3733">HADOOP-3733</a> appeared to be a minor problem with special characters in URLs. I hadn’t bothered to examine the source code, but I assumed it would be an easy fix. Who knows, maybe it would even give some eager young programmer the opportunity to make their first contribution to open-source.</p>
<p>I moved on; I wasn&#8217;t using Hadoop day-to-day anymore. About once a year, though, I got a reminder email from JIRA when someone else stumbled across the bug and chimed in. Three patches were submitted, with a brief discussion around each, but the bug remained unresolved. A clumsy workaround was suggested.</p>
<p><a href="http://www.catb.org/esr/writings/cathedral-bazaar/cathedral-bazaar/ar01s04.html" title="The Cathedral and the Bazaar: Release Early, Release Often">Linus’s Law</a> decrees that <em>Given enough eyeballs, all bugs are shallow.</em> But there&#8217;s a correlary: Given enough hands, all bugs are <em>trivial</em>. Which is not the same as <em>easy</em>.</p>
<p>The bug I reported clearly affected other people: It accumulated nine votes, making it the fourth-most-voted-on Hadoop ticket. And it seems like something easy to fix: just a simple character-escaping problem, a missed edge case. A beginning Java programmer should be able to fix it, right?</p>
<p>Perhaps that’s why no one <em>wanted</em> to fix it. HADOOP-3733 is not going to give anyone the opportunity to flex their algorithmic muscles or show off to their peers. It’s exactly the kind of tedious, persistent bug that programmers hate. It’s <em>boring</em>. And hey, there’s an easy workaround. <a href="https://en.wikipedia.org/wiki/Somebody_else%27s_problem">Somebody else</a> will fix it, right?</p>
<p>Eventually it <em>was</em> fixed. The final patch touched 12 files and added 724 lines: clearly non-trivial work requiring knowledge of Hadoop internals, a “deep” bug rather than a shallow one.</p>
<p>One day later, someone reported a <a href="https://issues.apache.org/jira/browse/HADOOP-13287">second bug</a> for the same issue with a different special character.</p>
<p><a href="https://xkcd.com/1700/"><img src="http://imgs.xkcd.com/comics/new_bug.png" title="XKCD comic: New Bug"></a></p>
<p>If there’s a lesson to draw from this, it’s that programming is not just hard, it’s often slow, tedious, and boring. It’s <em>work</em>. When programmers express a desire to contribute to open-source software, we think of grand designs, flashy new tools, and cheering crowds at conferences.</p>
<p>A reward system based on <a href="http://www.catb.org/esr/writings/cathedral-bazaar/cathedral-bazaar/ar01s11.html" title="The Cathedral and the Bazaar: The Social Context of Open-Source Software">ego satisfaction and reputation</a> optimizes for interesting, novel work. Everyone wants to be the master architect of the groundbreaking new framework in the hip new language. No one wants to dig through dozens of Java files for a years-old parsing bug.</p>
<p>But sometimes that’s the work that needs to be done.</p>
<p>* * *</p>
<p><strong>Edit 2016-07-19:</strong> The author of the final patch, Steve Loughran,  wrote up his analysis of the problem and its solution: <a href="http://steveloughran.blogspot.co.uk/2016/07/gardening-commons.html">Gardening the Commons</a>. He deserves a lot of credit for being willing to take the (considerable) time needed to dig into the details of such an old bug and then work out a solution that addresses the root cause.</p>
	</div><!-- .entry-content -->

	<footer class="entry-footer">
		<span class="byline"><span class="author vcard"><img alt='' src='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=49&amp;d=blank&amp;r=g' srcset='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=98&amp;d=blank&amp;r=g 2x' class='avatar avatar-49 photo' height='49' width='49' /><span class="screen-reader-text">Author </span> <a class="url fn n" href="../author/stuart.html">Stuart</a></span></span><span class="posted-on"><span class="screen-reader-text">Posted on </span><a href="../2016/07/18/apathy-of-the-commons.html" rel="bookmark"><time class="entry-date published" datetime="2016-07-18T08:22:13+00:00">July 18, 2016</time><time class="updated" datetime="2016-07-19T09:14:00+00:00">July 19, 2016</time></a></span><span class="cat-links"><span class="screen-reader-text">Categories </span><a href="../category/programming.html" rel="category tag">Programming</a></span><span class="tags-links"><span class="screen-reader-text">Tags </span><a href="hadoop.html" rel="tag">Hadoop</a>, <a href="open-source.html" rel="tag">open-source</a></span>			</footer><!-- .entry-footer -->
</article><!-- #post-## -->

<article id="post-419" class="post-419 post type-post status-publish format-standard hentry category-programming tag-clojure tag-hadoop">
	<header class="entry-header">
		
		<h2 class="entry-title"><a href="../2010/01/11/clojure-hadoop-1-0-0.html" rel="bookmark">Clojure-Hadoop 1.0.0</a></h2>	</header><!-- .entry-header -->

	
	
	<div class="entry-content">
		<p>At long last, I have made a formal release of my <a href="../2010/01/11/clojure-hadoop-1-0-0.html">clojure-hadoop</a> library.  Downloads and more information <a href="../2010/01/11/clojure-hadoop-1-0-0.html">here</a>.</p>
<p>The 1.0.0 release is documented, but not in exhaustive detail.  Other people have used this successfully, but it may not support all possible Hadoop configurations.</p>
<p>Watch video of <a href="http://vimeo.com/7669741">my presentation at HadoopWorld NYC</a>.</p>
	</div><!-- .entry-content -->

	<footer class="entry-footer">
		<span class="byline"><span class="author vcard"><img alt='' src='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=49&amp;d=blank&amp;r=g' srcset='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=98&amp;d=blank&amp;r=g 2x' class='avatar avatar-49 photo' height='49' width='49' /><span class="screen-reader-text">Author </span> <a class="url fn n" href="../author/stuart.html">Stuart</a></span></span><span class="posted-on"><span class="screen-reader-text">Posted on </span><a href="../2010/01/11/clojure-hadoop-1-0-0.html" rel="bookmark"><time class="entry-date published" datetime="2010-01-11T13:35:43+00:00">January 11, 2010</time><time class="updated" datetime="2010-01-11T13:36:47+00:00">January 11, 2010</time></a></span><span class="cat-links"><span class="screen-reader-text">Categories </span><a href="../category/programming.html" rel="category tag">Programming</a></span><span class="tags-links"><span class="screen-reader-text">Tags </span><a href="clojure.html" rel="tag">Clojure</a>, <a href="hadoop.html" rel="tag">Hadoop</a></span>			</footer><!-- .entry-footer -->
</article><!-- #post-## -->

<article id="post-246" class="post-246 post type-post status-publish format-standard hentry category-programming tag-altlaw tag-hadoop">
	<header class="entry-header">
		
		<h2 class="entry-title"><a href="../2009/06/26/big-small-at-the-same-time.html" rel="bookmark">Big &#038; Small at the same time</a></h2>	</header><!-- .entry-header -->

	
	
	<div class="entry-content">
		<p>I haven&#8217;t posted in a while &#8212; look for more later this summer.</p>
<p>But in the mean time, I have a question: How do you structure data such that you can efficiently manipulate it on both a large scale and a small scale at the same time?</p>
<p>By large scale, I mean applying a transformation or analysis efficiently over every record in a multi-gigabyte collection.  Hadoop is very good at this, but it achieves its efficiency by working with collections in large chunks, typically 64 MB and up.</p>
<p>What you can&#8217;t do with Hadoop &#8212; at least, not efficiently &#8212; is retrieve a single record.  Systems layered on top of Hadoop, like HBase, attempt to mitigate the problem, but they are still slower than, say, a relational database.</p>
<p>In fact, considering the history of storage and data access technologies, most of them have been geared toward efficient random access to individual records &#8212; RAM, filesystems, hard disks, RDBMS&#8217;s, etc.  But as Hadoop demonstrates, random-access systems tend to be inefficient for handling very large data collections <em>in the aggregate</em>.</p>
<p>This is not merely theoreticaly musing &#8212; it&#8217;s a problem I&#8217;m trying to solve with <a href="http://www.altlaw.org/">AltLaw</a>.  I can use Hadoop to process millions of small records.  The results come out in large Hadoop SequenceFiles.  But then I want to provide random-access to those records via the web site.  So I have to somehow &#8220;expand&#8221; the contents of those SequenceFiles into individual records and store those records in some format that provides efficient random access.</p>
<p>Right now, I use two very blunt instruments &#8212; Lucene indexes and plain old files.  In the final stage of my processing chain, metadata and searchable text get written to a Lucene index, and the pre-rendered HTML content of each page gets written to a file on an XFS filesystem.  This works, but it ends up being one of the slower parts of the process.  Building multiple Lucene indexes and merging them into one big (~6 GB) index takes an hour; writing all the files to the XFS filesystem takes about 20 minutes.  There is no interesting data manipulation going on here, I&#8217;m just moving data around.</p>
	</div><!-- .entry-content -->

	<footer class="entry-footer">
		<span class="byline"><span class="author vcard"><img alt='' src='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=49&amp;d=blank&amp;r=g' srcset='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=98&amp;d=blank&amp;r=g 2x' class='avatar avatar-49 photo' height='49' width='49' /><span class="screen-reader-text">Author </span> <a class="url fn n" href="../author/stuart.html">Stuart</a></span></span><span class="posted-on"><span class="screen-reader-text">Posted on </span><a href="../2009/06/26/big-small-at-the-same-time.html" rel="bookmark"><time class="entry-date published updated" datetime="2009-06-26T13:14:17+00:00">June 26, 2009</time></a></span><span class="cat-links"><span class="screen-reader-text">Categories </span><a href="../category/programming.html" rel="category tag">Programming</a></span><span class="tags-links"><span class="screen-reader-text">Tags </span><a href="altlaw.html" rel="tag">AltLaw</a>, <a href="hadoop.html" rel="tag">Hadoop</a></span><span class="comments-link"><a href="../2009/06/26/big-small-at-the-same-time.html#comments">4 Comments<span class="screen-reader-text"> on Big &#038; Small at the same time</span></a></span>			</footer><!-- .entry-footer -->
</article><!-- #post-## -->

<article id="post-207" class="post-207 post type-post status-publish format-standard hentry category-programming tag-altlaw tag-hadoop">
	<header class="entry-header">
		
		<h2 class="entry-title"><a href="../2009/02/05/207.html" rel="bookmark">Hadoop Meetup 2/10</a></h2>	</header><!-- .entry-header -->

	
	
	<div class="entry-content">
		<p>Just a little self-promotion: I&#8217;ll be presenting at the <a href="http://www.meetup.com/Hadoop-NYC/">New York Hadoop User Group</a> on Tuesday, February 10 at 6:30.  I&#8217;ll talk about how I use Hadoop for <a href="http://www.altlaw.org/">AltLaw.org</a>, including citation linking, distributed indexing, and using <a href="http://www.clojure.org/">Clojure</a> with Hadoop.</p>
<p><strong>Update 2/28:</strong> My <a href="http://files.meetup.com/1228907/altlaw-hadoop-meetup-2009-02-10-public.pdf">slides from this presentation</a> are available from the <a href="http://www.meetup.com/Hadoop-NYC/files/">Meetup group files</a>.</p>
	</div><!-- .entry-content -->

	<footer class="entry-footer">
		<span class="byline"><span class="author vcard"><img alt='' src='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=49&amp;d=blank&amp;r=g' srcset='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=98&amp;d=blank&amp;r=g 2x' class='avatar avatar-49 photo' height='49' width='49' /><span class="screen-reader-text">Author </span> <a class="url fn n" href="../author/stuart.html">Stuart</a></span></span><span class="posted-on"><span class="screen-reader-text">Posted on </span><a href="../2009/02/05/207.html" rel="bookmark"><time class="entry-date published" datetime="2009-02-05T13:30:11+00:00">February 5, 2009</time><time class="updated" datetime="2009-02-28T13:07:02+00:00">February 28, 2009</time></a></span><span class="cat-links"><span class="screen-reader-text">Categories </span><a href="../category/programming.html" rel="category tag">Programming</a></span><span class="tags-links"><span class="screen-reader-text">Tags </span><a href="altlaw.html" rel="tag">AltLaw</a>, <a href="hadoop.html" rel="tag">Hadoop</a></span><span class="comments-link"><a href="../2009/02/05/207.html#comments">4 Comments<span class="screen-reader-text"> on Hadoop Meetup 2/10</span></a></span>			</footer><!-- .entry-footer -->
</article><!-- #post-## -->

<article id="post-186" class="post-186 post type-post status-publish format-standard hentry category-programming tag-altlaw tag-hadoop tag-normalization tag-rdf">
	<header class="entry-header">
		
		<h2 class="entry-title"><a href="../2008/10/23/antidenormalizationism.html" rel="bookmark">Antidenormalizationism</a></h2>	</header><!-- .entry-header -->

	
	
	<div class="entry-content">
		<p>When storing any large collection of data, one of the most critical decisions one has to make is when to normalize and when to denormalize.  Normalized data is good for flexibility &#8212; you can write queries to recombine things in any combination.  Denormalized data is more efficient when you know, in advance, what the queries will be.  There&#8217;s a maxim, &#8220;normalize until it hurts, denormalize until it works.&#8221;</p>
<p>In the past two years, I&#8217;ve become a big fan of three software paradigms:</p>
<ol>
<li>RDF and the Semantic Web</li>
<li>RESTful architectures</li>
<li>Hadoop and MapReduce</li>
</ol>
<p>The first two were made for each other &#8212; RDF is the ideal data format for the Semantic Web, and Semantic Web applications are (ideally) implemented with RESTful architectures.</p>
<p>Unfortunately, as I discovered through painful experience, RDF is an inefficient representation for large amounts of data.  It&#8217;s the ultimate in normalization &#8212; each triple represents a single fact &#8212; and in flexibility, but even simple queries become prohibitively slow when each item of interest is spread across dozens of triples.</p>
<p>At the other end of the spectrum, MapReduce programming forces you into an entirely different way of thinking.  MapReduce offers exactly one data access mechanism: read a list of records from beginning to end.  No random access, do not pass GO, do not collect $200.  That restriction is what enables Hadoop to distribute a job across many machines, and to process data at the maximum rate supported by the hard disk.  Obviously, to take full advantage of these optimizations, your MapReduce program needs to be able to process each record in isolation, without referring to any other resources.  In effect, everything has to be completely denormalized.</p>
<p>For my work on <a href="http://altlaw.org/">AltLaw</a>, I&#8217;ve tended to use fully denormalized data, because anything else is too slow.  But I <em>want</em> to be working with normalized data.  I want to have every datum &#8212; plus information about its derivation &#8212; stored in one giant RDF graph.  But I also want to be able to process this graph efficiently.  Maybe if I had a dozen machines with 64 GB of memory apiece, this wouldn&#8217;t be a problem.  But with one desktop, one server, and occasional rides on EC2, that&#8217;s not an option.</p>
<p>The ideal design, I think, would be a hybrid system which can do efficient bulk processing of RDF data.  There are some pilot projects to do this with Hadoop, and I&#8217;m interested to see how they pan out.  But until then, I&#8217;ll have to make do as best I can.</p>
<p><strong>Update:</strong> The projects I remebered were HRDF and Heart, which turn out to be the same thing: <a href="http://code.google.com/p/hrdf/">http://code.google.com/p/hrdf/</a> and <a href="http://wiki.apache.org/incubator/HeartProposal">http://wiki.apache.org/incubator/HeartProposal</a></p>
	</div><!-- .entry-content -->

	<footer class="entry-footer">
		<span class="byline"><span class="author vcard"><img alt='' src='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=49&amp;d=blank&amp;r=g' srcset='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=98&amp;d=blank&amp;r=g 2x' class='avatar avatar-49 photo' height='49' width='49' /><span class="screen-reader-text">Author </span> <a class="url fn n" href="../author/stuart.html">Stuart</a></span></span><span class="posted-on"><span class="screen-reader-text">Posted on </span><a href="../2008/10/23/antidenormalizationism.html" rel="bookmark"><time class="entry-date published" datetime="2008-10-23T17:24:15+00:00">October 23, 2008</time><time class="updated" datetime="2008-10-24T13:58:05+00:00">October 24, 2008</time></a></span><span class="cat-links"><span class="screen-reader-text">Categories </span><a href="../category/programming.html" rel="category tag">Programming</a></span><span class="tags-links"><span class="screen-reader-text">Tags </span><a href="altlaw.html" rel="tag">AltLaw</a>, <a href="hadoop.html" rel="tag">Hadoop</a>, <a href="normalization.html" rel="tag">normalization</a>, <a href="rdf.html" rel="tag">RDF</a></span><span class="comments-link"><a href="../2008/10/23/antidenormalizationism.html#comments">3 Comments<span class="screen-reader-text"> on Antidenormalizationism</span></a></span>			</footer><!-- .entry-footer -->
</article><!-- #post-## -->

<article id="post-171" class="post-171 post type-post status-publish format-standard hentry category-programming tag-altlaw tag-hadoop tag-thrift">
	<header class="entry-header">
		
		<h2 class="entry-title"><a href="../2008/07/17/the-document-blob-model.html" rel="bookmark">The Document-Blob Model</a></h2>	</header><!-- .entry-header -->

	
	
	<div class="entry-content">
		<p><strong>Update September 22, 2008:</strong> I have abandoned this model.  I&#8217;m still using Hadoop, but with a much simpler data model.  I&#8217;ll post about it at some point.</p>
<p>&#8230;</p>
<p>Gosh darn, it&#8217;s hard to get this right.  In my most recent work on <a href="http://altlaw.org/">AltLaw</a>, I&#8217;ve been building an infrastructure for doing all my back-end data processing using <a href="http://hadoop.apache.org/core/">Hadoop</a> and <a href="http://incubator.apache.org/thrift/">Thrift</a>.</p>
<p>I&#8217;ve described this before, but here&#8217;s a summary of my situation:</p>
<ol>
<li>I have a few million files, ~15 GB total.</li>
<li>Many files represent the same logical entity, sometimes in different formats or from different sources.</li>
<li>Every file needs 5-10 steps of clean-up, data extraction, and format conversion.</li>
<li>The files come from ~15 different sources, each requiring different processing.</li>
<li>I get 20-50 new files daily.</li>
</ol>
<p>I want to be able to process all this efficiently, but I also want to be able to change my mind.  I can never say, &#8220;I&#8217;ve run this process on this batch of files, so I never need to do it again.&#8221;  I might improve the code, or I might find that I need to go back to the original files to get some other kind of data.</p>
<p>Hadoop was the obvious choice for efficiency.  Thrift is a good, compact data format that&#8217;s easier to use than Hadoop&#8217;s native data structures.  The only question is, what&#8217;s the schema?  Or, more simply, what do I want to store?</p>
<p>I&#8217;ve come up with what, for want of a better term, I call the <strong>Document-Blob Model</strong>.</p>
<p>I start with a collection of Documents.  Each Document represents a single, logical entity, like a court case or a section of statute.  A Document contains an integer identifier and an array of Blobs, nothing more.</p>
<p>What is a Blob?  Good question.  It&#8217;s data, any data.  It may represent a normal file, in which case it stores the content of that file and some metadata like the MIME type.  It may also represent a data structure.  Because unused fields do not occupy any space in Thrift&#8217;s binary format, the Blob type can have fields for every structure I might want to use now or in the future.  In effect, a Blob is a polymorphic type that can become any other type.</p>
<p>So how do I know which type it is?  By where it came from.  Each Blob is tagged with the name of its &#8220;provider&#8221;.  For files downloaded in bulk, the provider is the web site or service where I got them.  For generated files, the creator is a class or script, with a version number.</p>
<p>So I have a few hundred thousand Documents, each containing several Blobs.  I represent each conversion/extraction/processing step as its own Java class.  All those classes implement the same, simple interface: take one Blob as input and return another Blob as output.</p>
<p>Helper functions allow me to say things like, &#8220;Take this Document, find the Blob that was generated by class X.  Run  class Y on that Blob, and append result to the original Document.&#8221;  In this way, I can stack multiple processing steps into a single Hadoop job, but retain the option of reusing or rearranging those steps later on.</p>
<p>Will this work?  I have no idea.  I just came up with it last week.  Today I successfully ran a 5-step job on ~700,000 documents from the <a href="http://bulk.resource.org/courts.gov/c/">public.resource.org federal case corpus</a>.  It took about an hour on a 10-node Hadoop/EC2 cluster.</p>
<p>The real test will come when I apply this model to the much messier collection of files we downloaded directly from the federal courts.</p>
	</div><!-- .entry-content -->

	<footer class="entry-footer">
		<span class="byline"><span class="author vcard"><img alt='' src='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=49&amp;d=blank&amp;r=g' srcset='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=98&amp;d=blank&amp;r=g 2x' class='avatar avatar-49 photo' height='49' width='49' /><span class="screen-reader-text">Author </span> <a class="url fn n" href="../author/stuart.html">Stuart</a></span></span><span class="posted-on"><span class="screen-reader-text">Posted on </span><a href="../2008/07/17/the-document-blob-model.html" rel="bookmark"><time class="entry-date published" datetime="2008-07-17T16:43:23+00:00">July 17, 2008</time><time class="updated" datetime="2008-09-22T12:39:56+00:00">September 22, 2008</time></a></span><span class="cat-links"><span class="screen-reader-text">Categories </span><a href="../category/programming.html" rel="category tag">Programming</a></span><span class="tags-links"><span class="screen-reader-text">Tags </span><a href="altlaw.html" rel="tag">AltLaw</a>, <a href="hadoop.html" rel="tag">Hadoop</a>, <a href="thrift.html" rel="tag">Thrift</a></span><span class="comments-link"><a href="../2008/07/17/the-document-blob-model.html#comments">3 Comments<span class="screen-reader-text"> on The Document-Blob Model</span></a></span>			</footer><!-- .entry-footer -->
</article><!-- #post-## -->

<article id="post-157" class="post-157 post type-post status-publish format-standard hentry category-programming tag-aws tag-ec2 tag-hadoop">
	<header class="entry-header">
		
		<h2 class="entry-title"><a href="../2008/05/14/ec2-authorizations-for-hadoop.html" rel="bookmark">EC2 Authorizations for Hadoop</a></h2>	</header><!-- .entry-header -->

	
	
	<div class="entry-content">
		<p>I just did my first test-run of a Hadoop cluster on Amazon EC2.  It&#8217;s not as tricky as it appears, although I ran into some snags, which I&#8217;ll document here.  I also found these pages helpful: <a href="http://wiki.apache.org/hadoop/AmazonEC2">EC2 on Hadoop Wiki</a> and <a href="http://www.manamplified.org/archives/2008/03/notes-on-using-ec2-s3.html">manAmplified</a>.</p>
<p>First, make sure the <a href="http://developer.amazonwebservices.com/connect/entry.jspa?externalID=351&amp;categoryID=88">EC2 API tools</a> are installed and on your path.  Also make sure the EC2 environment variables are set.  I added the following to my <code>~/.bashrc</code>:</p>
<pre>export EC2_HOME=$HOME/ec2-api-tools-1.3-19403
export EC2_PRIVATE_KEY=$HOME/.ec2/MY_PRIVATE_KEY_FILE
export EC2_CERT=$HOME/.ec2/MY_CERT_FILE
export PATH=$PATH:$EC2_HOME/bin</pre>
<p>I also copied my generated SSH key to <code>~/.ec2/id_rsa-MY_KEY_NAME</code>.</p>
<p>You need authorizations for the EC2 security group that Hadoop uses.  The scripts in <code>hadoop-*/src/contrib/ec2</code> are supposed to do this for you, but they didn&#8217;t for me.  I had to do:</p>
<pre>ec2-add-group hadoop-cluster-group -d "Group for Hadoop clusters."
ec2-authorize hadoop-cluster-group -p 22
ec2-authorize hadoop-cluster-group -o hadoop-cluster-group -u YOUR_AWS_ACCOUNT_ID
ec2-authorize hadoop-cluster-group -p 50030
ec2-authorize hadoop-cluster-group -p 50060</pre>
<p>The first line creates the security group.  The second line lets you SSH into it.  The third line lets the individual nodes in the cluster communicate with one another.  The fourth and fifth lines are optional; they let you monitor your MapReduce jobs through Hadoop&#8217;s web interface.  (If you have a fixed IP address, you can be slightly more secure by adding <code>-s YOUR_ADDRESS</code> to the commands above.)</p>
<p>These authorizations are permanently tied to your AWS account, not to any particular group of instances, so you only need to do this once.  You can see your current EC2 authorization settings with ec2-describe-group, it should look something like this:</p>
<pre>GROUP   YOUR_AWS_ID    hadoop-cluster-group    Group for Hadoop clusters.
PERMISSION      YOUR_AWS_ID    hadoop-cluster-group    ALLOWS  all                     FROM    USER    YOUR_AWS_ID    GRPNAME hadoop-cluster-group
PERMISSION      YOUR_AWS_ID    hadoop-cluster-group    ALLOWS  tcp     22      22      FROM    CIDR    0.0.0.0/0</pre>
<p>With additional lines for ports 50030 and 50060, if you enabled those.</p>
	</div><!-- .entry-content -->

	<footer class="entry-footer">
		<span class="byline"><span class="author vcard"><img alt='' src='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=49&amp;d=blank&amp;r=g' srcset='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=98&amp;d=blank&amp;r=g 2x' class='avatar avatar-49 photo' height='49' width='49' /><span class="screen-reader-text">Author </span> <a class="url fn n" href="../author/stuart.html">Stuart</a></span></span><span class="posted-on"><span class="screen-reader-text">Posted on </span><a href="../2008/05/14/ec2-authorizations-for-hadoop.html" rel="bookmark"><time class="entry-date published updated" datetime="2008-05-14T11:58:26+00:00">May 14, 2008</time></a></span><span class="cat-links"><span class="screen-reader-text">Categories </span><a href="../category/programming.html" rel="category tag">Programming</a></span><span class="tags-links"><span class="screen-reader-text">Tags </span><a href="aws.html" rel="tag">AWS</a>, <a href="ec2.html" rel="tag">EC2</a>, <a href="hadoop.html" rel="tag">Hadoop</a></span><span class="comments-link"><a href="../2008/05/14/ec2-authorizations-for-hadoop.html#comments">1 Comment<span class="screen-reader-text"> on EC2 Authorizations for Hadoop</span></a></span>			</footer><!-- .entry-footer -->
</article><!-- #post-## -->

<article id="post-151" class="post-151 post type-post status-publish format-standard hentry category-programming tag-hadoop">
	<header class="entry-header">
		
		<h2 class="entry-title"><a href="../2008/04/24/a-million-little-files.html" rel="bookmark">A Million Little Files</a></h2>	</header><!-- .entry-header -->

	
	
	<div class="entry-content">
		<p>My PC-oriented brain says it&#8217;s easier to work with a million small files than one gigantic file.  Hadoop says the opposite &#8212; big files are stored contiguously on disk, so they can be read/written efficiently.  UNIX tar files work on the same principle, but Hadoop can&#8217;t read them directly because they don&#8217;t contain enough information for efficient splitting.  So, I wrote a program to convert tar files into Hadoop sequence files.</p>
<p>Here&#8217;s some code (Apache license), including all the Apache jars needed to make it work:</p>
<p><a href="../download/tar-to-seq.tar.tar">tar-to-seq.tar.gz</a> (6.1 MB)</p>
<p>Unpack it and run:</p>
<pre>
java -jar tar-to-seq.jar <em>tar-file</em> <em>sequence-file</em>
</pre>
<p>The output sequence file is BLOCK-compressed, about 1.4 times the size of a bzip2-compressed tar file.  Each key is the name of a file (a Hadoop &#8220;Text&#8221;), the value is the binary contents of the file (a BytesWritable).</p>
<p>It took about an hour and a half to convert a 615MB tar.bz2 file to an 868MB sequence file.  That&#8217;s slow, but it only has to be done once.</p>
	</div><!-- .entry-content -->

	<footer class="entry-footer">
		<span class="byline"><span class="author vcard"><img alt='' src='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=49&amp;d=blank&amp;r=g' srcset='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=98&amp;d=blank&amp;r=g 2x' class='avatar avatar-49 photo' height='49' width='49' /><span class="screen-reader-text">Author </span> <a class="url fn n" href="../author/stuart.html">Stuart</a></span></span><span class="posted-on"><span class="screen-reader-text">Posted on </span><a href="../2008/04/24/a-million-little-files.html" rel="bookmark"><time class="entry-date published" datetime="2008-04-24T11:25:43+00:00">April 24, 2008</time><time class="updated" datetime="2008-04-24T11:37:00+00:00">April 24, 2008</time></a></span><span class="cat-links"><span class="screen-reader-text">Categories </span><a href="../category/programming.html" rel="category tag">Programming</a></span><span class="tags-links"><span class="screen-reader-text">Tags </span><a href="hadoop.html" rel="tag">Hadoop</a></span><span class="comments-link"><a href="../2008/04/24/a-million-little-files.html#comments">23 Comments<span class="screen-reader-text"> on A Million Little Files</span></a></span>			</footer><!-- .entry-footer -->
</article><!-- #post-## -->

<article id="post-146" class="post-146 post type-post status-publish format-standard hentry category-programming tag-hadoop">
	<header class="entry-header">
		
		<h2 class="entry-title"><a href="../2008/04/17/disk-is-the-new-tape.html" rel="bookmark">Disk is the New Tape</a></h2>	</header><!-- .entry-header -->

	
	
	<div class="entry-content">
		<p>An interesting scenario from <a href="http://en.wikipedia.org/wiki/Doug_Cutting">Doug Cutting</a>: Say you have a terabyte of data, on a disk with 10ms seek time and 100MB/s max throughput.  You want to update 1% of the records.  If you do it with random-access seeks, it takes 35 days to finish.  On the other hand, if you scan the entire database sequentially and write it back out again, it takes 5.6 hours.</p>
<p>This is why <a href="http://hadoop.apache.org/">Hadoop</a> only supports <a href="http://mail-archives.apache.org/mod_mbox/hadoop-core-user/200804.mbox/%3cC41A95D5.3B99B%25tdunning@veoh.com%3e">linear access to the filesystem</a>.  It&#8217;s also why Hadoop coder Tom White says <a href="http://www.lexemetech.com/2008/03/disks-have-become-tapes.html">disks have become tapes</a>.  All this is contrary to the way I think about data &#8212; building hash tables and indexes to do fast random-access lookups.  I&#8217;m still trying to get my head around this concept of &#8220;linear&#8221; data processing.  But I have found that I can do some things faster by reading sequentially through a batch of files rather than trying to stuff everything in a database (RDF or SQL) and doing big join queries.</p>
	</div><!-- .entry-content -->

	<footer class="entry-footer">
		<span class="byline"><span class="author vcard"><img alt='' src='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=49&amp;d=blank&amp;r=g' srcset='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=98&amp;d=blank&amp;r=g 2x' class='avatar avatar-49 photo' height='49' width='49' /><span class="screen-reader-text">Author </span> <a class="url fn n" href="../author/stuart.html">Stuart</a></span></span><span class="posted-on"><span class="screen-reader-text">Posted on </span><a href="../2008/04/17/disk-is-the-new-tape.html" rel="bookmark"><time class="entry-date published updated" datetime="2008-04-17T12:50:11+00:00">April 17, 2008</time></a></span><span class="cat-links"><span class="screen-reader-text">Categories </span><a href="../category/programming.html" rel="category tag">Programming</a></span><span class="tags-links"><span class="screen-reader-text">Tags </span><a href="hadoop.html" rel="tag">Hadoop</a></span><span class="comments-link"><a href="../2008/04/17/disk-is-the-new-tape.html#comments">4 Comments<span class="screen-reader-text"> on Disk is the New Tape</span></a></span>			</footer><!-- .entry-footer -->
</article><!-- #post-## -->

<article id="post-145" class="post-145 post type-post status-publish format-standard hentry category-programming tag-altlaw tag-cascading tag-hadoop">
	<header class="entry-header">
		
		<h2 class="entry-title"><a href="../2008/04/17/continuous-integration-for-data.html" rel="bookmark">Continuous Integration for Data</a></h2>	</header><!-- .entry-header -->

	
	
	<div class="entry-content">
		<p>As I told a friend recently, I&#8217;m pretty happy with the front-end code of <a href="http://altlaw.org/">AltLaw</a>.  It&#8217;s just a simple Ruby on Rails app that uses Solr for search and storage.  The code is small and easy to maintain.</p>
<p>What I&#8217;m <em>not</em> happy with is the back-end code, the data extraction, formatting, and indexing.  It&#8217;s a hodge-podge of Ruby, Perl, <a href="http://clojure.sourceforge.net/">Clojure</a>, C, shell scripts, SQL, XML, RDF, and text files that could make the most dedicated Unix hacker blanch.  It works, but just barely, and I panic every time I think about implementing a new feature.</p>
<p>This is a software engineering problem rather than a pure computer science problem, and I never pretended to be a software engineer.  (I never pretended to be a computer scientist, either.)  It might also be a problem for a larger team than an army of one (plus a few volunteers).</p>
<p>But given that I can get more processing power (via Amazon) more easily than I can get more programmers, how can I make use of the resources I have to enhance my own productivity?</p>
<p>I&#8217;m studying <a href="http://hadoop.apache.org/core/">Hadoop</a> and <a href="http://www.cascading.org/">Cascading</a> in the hopes that they will help.  But those systems are inherently batch-oriented.  I&#8217;d like to move away from a batch processing model if I can.  Given that AltLaw acquires 50 to 100 new cases per day, adding to a growing database of over half a million, what I would really like to have is a kind of &#8220;continuous integration&#8221; process for data.  I want a server that runs continuously, accepting new data and new code and automatically re-running processes as needed to keep up with dependencies.  Perhaps given a year of free time I could invent this myself, but I&#8217;m too busy debugging my shell scripts.</p>
	</div><!-- .entry-content -->

	<footer class="entry-footer">
		<span class="byline"><span class="author vcard"><img alt='' src='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=49&amp;d=blank&amp;r=g' srcset='https://secure.gravatar.com/avatar/55878d0196b91803f9cb2c372b0551d3?s=98&amp;d=blank&amp;r=g 2x' class='avatar avatar-49 photo' height='49' width='49' /><span class="screen-reader-text">Author </span> <a class="url fn n" href="../author/stuart.html">Stuart</a></span></span><span class="posted-on"><span class="screen-reader-text">Posted on </span><a href="../2008/04/17/continuous-integration-for-data.html" rel="bookmark"><time class="entry-date published updated" datetime="2008-04-17T10:57:02+00:00">April 17, 2008</time></a></span><span class="cat-links"><span class="screen-reader-text">Categories </span><a href="../category/programming.html" rel="category tag">Programming</a></span><span class="tags-links"><span class="screen-reader-text">Tags </span><a href="altlaw.html" rel="tag">AltLaw</a>, <a href="cascading.html" rel="tag">Cascading</a>, <a href="hadoop.html" rel="tag">Hadoop</a></span>			</footer><!-- .entry-footer -->
</article><!-- #post-## -->

		</main><!-- .site-main -->
	</div><!-- .content-area -->


	<aside id="secondary" class="sidebar widget-area" role="complementary">
		<section id="search-3" class="widget widget_search">
<form role="search" method="get" class="search-form" action="https://stuartsierra.com/">
	<label>
		<span class="screen-reader-text">Search for:</span>
		<input type="search" class="search-field" placeholder="Search &hellip;" value="" name="s" />
	</label>
	<button type="submit" class="search-submit"><span class="screen-reader-text">Search</span></button>
</form>
</section><section id="text-4" class="widget widget_text">			<div class="textwidget"><ul>
<li><a href="http://feeds2.feedburner.com/StuartSierra" title="Syndicate this site using RSS 2.0">Entries <abbr title="Really Simple Syndication">RSS</abbr></a></li>
<li><a href="../comments/feed" title="The latest comments to all posts in RSS">Comments <abbr title="Really Simple Syndication">RSS</abbr></a></li>
</ul>
<ul>
<li><a href="https://twitter.com/stuartsierra">Twitter</a></li>
<li><a href="https://github.com/stuartsierra">GitHub</a></li>
</ul></div>
		</section><section id="tag-widget-2" class="widget TagWidget"><h2 class="widget-title">Clojure Do’s and Don’ts</h2><ul class = "posts-by-tag-list"><li class="posts-by-tag-item Clojure do's and don'ts" id="posts-by-tag-item-883"><a class = "posts-by-tag-item-title" href="../2015/08/25/clojure-donts-lazy-effects.html">Clojure Don’ts: Lazy Effects</a></li><li class="posts-by-tag-item Clojure do's and don'ts" id="posts-by-tag-item-877"><a class = "posts-by-tag-item-title" href="../2015/08/10/clojure-donts-redundant-map.html">Clojure Don’ts: Redundant map</a></li><li class="posts-by-tag-item Clojure do's and don'ts" id="posts-by-tag-item-870"><a class = "posts-by-tag-item-title" href="../2015/06/16/clojure-donts-single-branch-if.html">Clojure Don’ts: Single-branch if</a></li><li class="posts-by-tag-item Clojure do's and don'ts" id="posts-by-tag-item-854"><a class = "posts-by-tag-item-title" href="../2015/06/10/clojure-donts-heisenparameter.html">Clojure Don’ts: The Heisenparameter</a></li><li class="posts-by-tag-item Clojure do's and don'ts" id="posts-by-tag-item-849"><a class = "posts-by-tag-item-title" href="../2015/06/01/clojure-donts-optional-arguments-with-varargs.html">Clojure Don’ts: Optional Arguments with Varargs</a></li><li class="posts-by-tag-item Clojure do's and don'ts errors" id="posts-by-tag-item-836"><a class = "posts-by-tag-item-title" href="../2015/05/27/clojure-uncaught-exceptions.html">Clojure Do’s: Uncaught Exceptions</a></li><li class="posts-by-tag-item Clojure do's and don'ts" id="posts-by-tag-item-831"><a class = "posts-by-tag-item-title" href="../2015/05/17/clojure-record-constructors.html">Record Constructors</a></li><li class="posts-by-tag-item Clojure do's and don'ts" id="posts-by-tag-item-826"><a class = "posts-by-tag-item-title" href="../2015/05/10/clojure-namespace-aliases.html">Clojure Do’s: Namespace Aliases</a></li><li class="posts-by-tag-item Clojure do's and don'ts" id="posts-by-tag-item-820"><a class = "posts-by-tag-item-title" href="../2015/05/02/clojure-donts-isa.html">Clojure Don’ts: isa?</a></li><li class="posts-by-tag-item Clojure do's and don'ts" id="posts-by-tag-item-812"><a class = "posts-by-tag-item-title" href="../2015/04/26/clojure-donts-concat.html">Clojure Don’ts: Concat</a></li></ul></section>	</aside><!-- .sidebar .widget-area -->

		</div><!-- .site-content -->

		<footer id="colophon" class="site-footer" role="contentinfo">
							<nav class="main-navigation" role="navigation" aria-label="Footer Primary Menu">
					<div class="menu-main-menu-container"><ul id="menu-main-menu-1" class="primary-menu"><li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-635"><a href="../about-me.html">About Me</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-633"><a href="../writing.html">Writing</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-634"><a href="../presentations.html">Presentations</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-636"><a href="../software.html">Software</a></li>
<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-637"><a href="../contact.html">Contact</a></li>
</ul></div>				</nav><!-- .main-navigation -->
			
			
			<div class="site-info">
								<span class="site-title"><a href="../index.html" rel="home">Digital Digressions by Stuart Sierra</a></span>
				<a href="https://wordpress.org/">Proudly powered by WordPress</a>
			</div><!-- .site-info -->
		</footer><!-- .site-footer -->
	</div><!-- .site-inner -->
</div><!-- .site -->

	<div style="display:none">
	<div class="grofile-hash-map-55878d0196b91803f9cb2c372b0551d3">
	</div>
	</div>
<script type='text/javascript' src='https://s0.wp.com/wp-content/js/devicepx-jetpack.js?ver=201635'></script>
<script type='text/javascript' src='https://secure.gravatar.com/js/gprofiles.js?ver=2016Sepaa'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var WPGroHo = {"my_hash":""};
/* ]]> */
</script>
<script type='text/javascript' src='../wp-content/plugins/jetpack/modules/wpgroho167b.js?ver=4.6'></script>
<script type='text/javascript' src='../wp-content/themes/twentysixteen/js/skip-link-focus-fix8de4.js?ver=20160816'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var screenReaderText = {"expand":"expand child menu","collapse":"collapse child menu"};
/* ]]> */
</script>
<script type='text/javascript' src='../wp-content/themes/twentysixteen/js/functions8de4.js?ver=20160816'></script>
<script type='text/javascript' src='../wp-includes/js/wp-embed.min167b.js?ver=4.6'></script>
<script type='text/javascript' src='https://stats.wp.com/e-201635.js' async defer></script>
<script type='text/javascript'>
	_stq = window._stq || [];
	_stq.push([ 'view', {v:'ext',j:'1:4.2.2',blog:'2702563',post:'0',tz:'-4',srv:'stuartsierra.com'} ]);
	_stq.push([ 'clickTrackerInit', '2702563', '0' ]);
</script>
</body>

<!-- Mirrored from stuartsierra.com/tag/hadoop by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 02 Sep 2016 16:56:09 GMT -->
</html>

<!-- Dynamic page generated in 0.385 seconds. -->
<!-- Cached page generated by WP-Super-Cache on 2016-09-02 12:55:09 -->

<!-- Compression = gzip -->